# ä½¿ç”¨transformerå®ç°OCRå­—ç¬¦è¯†åˆ« 
- **Writer** : æ°¸é©»
- **Date** : 2021.10.24

æœ¬æ¬¡ä»»åŠ¡ä»¥ICDAR2015 Incidental Scene Textä¸­çš„[Task4.3:word recognition](https://rrc.cvc.uab.es/?ch=4&com=downloads)å•è¯è¯†åˆ«ä¸ºå­ä»»åŠ¡ä½œä¸ºæ•°æ®é›†ã€‚è®²è§£å¦‚ä½•**ä½¿ç”¨transformeræ¥å®ç°ä¸€ä¸ªç®€å•çš„OCRæ–‡å­—è¯†åˆ«ä»»åŠ¡**ï¼Œå¹¶ä»ä¸­ä½“ä¼štransformeræ˜¯å¦‚ä½•åº”ç”¨åˆ°é™¤åˆ†ç±»ä»¥å¤–æ›´å¤æ‚çš„CVä»»åŠ¡ä¸­çš„ã€‚

æ–‡ç« ä»ä»¥ä¸‹å‡ ä¸ªæ–¹é¢è¿›è¡Œè®²è§£ï¼š
- æ•°æ®é›†ç®€ä»‹
- æ•°æ®é›†åˆ†æä¸å­—ç¬¦æ˜ å°„å…³ç³»æ„å»º
- å¦‚ä½•å°†transformerå¼•å…¥OCR
- æ„å»ºè®­ç»ƒæ¡†æ¶

åŒ…å«ä»¥ä¸‹å‡ æ–‡ä»¶ï¼š
- my_transformer.py ï¼ˆä¸Šæ–‡ä¸­æ„å»ºçš„transformerï¼‰
- analysis_recognition_dataset.pyï¼ˆæ•°æ®åˆ†æï¼‰
- train_utils.pyï¼ˆè®­ç»ƒè¾…åŠ©å‡½æ•°ï¼‰
- ocr_by_transformer.py (OCRä»»åŠ¡è®­ç»ƒè„šæœ¬)

ä¸‹é¢è¿›å…¥æ­£å¼å†…å®¹ï¼š
***
## æ•°æ®é›†ç®€ä»‹
æœ¬æ–‡OCRå®éªŒä½¿ç”¨çš„æ•°æ®é›†åŸºäº`ICDAR2015 Incidental Scene Text` ä¸­çš„ `Task 4.3: Word Recognition`ï¼Œè¿™æ˜¯ä¸€ä¸ªç®€å•çš„å•è¯è¯†åˆ«ä»»åŠ¡ã€‚
æ•°æ®é›†åœ°å€ï¼š[ä¸‹è½½é“¾æ¥](https://pan.baidu.com/s/1TIvYgkn_Q5Z9Nl0amwGZzg)
æå–ç ï¼šqa8v
è¯¥æ•°æ®é›†åŒ…å«äº†ä¼—å¤šè‡ªç„¶åœºæ™¯å›¾åƒä¸­å‡ºç°çš„æ–‡å­—åŒºåŸŸï¼ŒåŸå§‹æ•°æ®ä¸­è®­ç»ƒé›†å«æœ‰4468å¼ å›¾åƒï¼Œæµ‹è¯•é›†å«æœ‰2077å¼ å›¾åƒï¼Œä»–ä»¬éƒ½æ˜¯ä»åŸå§‹å¤§å›¾ä¸­ä¾æ®æ–‡å­—åŒºåŸŸçš„bounding boxè£å‰ªå‡ºæ¥çš„ï¼Œå›¾åƒä¸­çš„æ–‡å­—åŸºæœ¬å¤„äºå›¾ç‰‡ä¸­å¿ƒä½ç½®ã€‚

æ•°æ®é›†ä¸­çš„å›¾åƒç±»ä¼¼å¦‚ä¸‹æ ·å¼ï¼š
|word_34.png "SHOP"  |word_241.png "Line" |
| ------ | ------ |
|  ![34](./img/word_34.png) | ![241](./img/word_241.png) | 

æ•°æ®é›†ç»“æ„å¤§è‡´å¦‚ä¸‹
- train
- train_gt.txt
- valid
- valid.txt

ä¸ºäº†ç®€åŒ–åç»­å®éªŒçš„è¯†åˆ«éš¾åº¦ï¼Œæä¾›çš„æ•°æ®é›†**ä½¿ç”¨é«˜å®½æ¯”>1.5ç²—ç•¥è¿‡æ»¤äº†æ–‡å­—ç«–å‘æ’åˆ—çš„å›¾åƒ**ï¼Œå› æ­¤ä¸ICDAR2015çš„åŸå§‹æ•°æ®é›†ç•¥æœ‰å·®åˆ«ã€‚

## æ•°æ®åˆ†æä¸å­—ç¬¦æ˜ å°„å…³ç³»æ„å»º
æ„å»ºanalysis_recognition_dataset.pyè„šæœ¬æ¥å¯¹æ•°æ®è¿›è¡Œç®€å•åˆ†æ
è¿™ä¸ªè„šæœ¬çš„ä½œç”¨æ˜¯ï¼š
- å¯¹æ•°æ®è¿›è¡Œæ ‡ç­¾å­—ç¬¦ç»Ÿè®¡
- æœ€é•¿æ ‡ç­¾é•¿åº¦ç»Ÿè®¡
- å›¾åƒå°ºå¯¸åˆ†æ
- æ„å»ºå­—ç¬¦æ ‡ç­¾çš„æ˜ å°„å…³ç³»æ–‡ä»¶`lbl2id_map.txt`

é¦–å…ˆè¿›è¡Œå‡†å¤‡å·¥ä½œï¼Œå¯¼å…¥éœ€è¦çš„åº“ï¼Œè®¾ç½®ç›¸å…³è·¯å¾„
```python
import os  
import cv2
base_data_dir = '../ICDAR_2015/'

train_img_dir = os.path.join(base_data_dir, 'train')
valid_img_dir = os.path.join(base_data_dir, 'valid')
train_lbl_path = os.path.join(base_data_dir, 'train_gt.txt')
valid_lbl_path = os.path.join(base_data_dir, 'valid_gt.txt')
lbl2id_map_path = os.path.join(base_data_dir, 'lbl2id_map.txt')
```

### 1ã€ç»Ÿè®¡æ ‡ç­¾æ–‡ä»¶ä¸­éƒ½åŒ…å«å“ªäº›labelä»¥åŠå„è‡ªå‡ºç°çš„æ¬¡æ•°
```python
def statistics_label_cnt(lbl_path, lbl_cnt_map):
    """
    ç»Ÿè®¡æ ‡ç­¾æ–‡ä»¶ä¸­éƒ½åŒ…å«å“ªäº›labelä»¥åŠå„è‡ªå‡ºç°çš„æ¬¡æ•°
    """
    with open(lbl_path, 'r', encoding='utf-8') as reader:
        for line in reader:
            items = line.rstrip().split(',')
            img_name = items[0]
            lbl_str = items[1].strip()[1:-1]
            for lbl in lbl_str:
                if lbl not in lbl_cnt_map.keys():
                    lbl_cnt_map[lbl] = 1
                else:
                    lbl_cnt_map[lbl] += 1
```

### 2ã€ç»Ÿè®¡æ ‡ç­¾æ–‡ä»¶ä¸­æœ€é•¿çš„labelæ‰€åŒ…å«çš„å­—ç¬¦æ•°
```python
def statistics_max_len_label(lbl_path):
    """
    ç»Ÿè®¡æ ‡ç­¾æ–‡ä»¶ä¸­æœ€é•¿çš„labelæ‰€åŒ…å«çš„å­—ç¬¦æ•°
    """
    max_len = -1
    with open(lbl_path, 'r', encoding='utf-8') as reader:
        for line in reader:
            items = line.rstrip().split(',')
            img_name = items[0]
            lbl_str = items[1].strip()[1:-1]
            lbl_len = len(lbl_str)
            max_len = max_len if max_len > lbl_len else lbl_len
    return max_len
```
å…·ä½“æ•ˆæœå¦‚ä¸‹ï¼š
```
æ•°æ®é›†ä¸­åŒ…å«å­—ç¬¦æœ€å¤šçš„labelé•¿åº¦ä¸º21
è®­ç»ƒé›†ä¸­å‡ºç°çš„label
{'[': 2, '0': 182, '6': 38, ']': 2, '2': 119, '-': 68, '3': 50, 'C': 593, 'a': 843, 'r': 655, 'p': 197, 'k': 96, 'E': 1421, 'X': 110, 'I': 861, 'T': 896, 'R': 836, 'f': 133, 'u': 293, 's': 557, 'i': 651, 'o': 659, 'n': 605, 'l': 408, 'e': 1055, 'v': 123, 'A': 1189, 'U': 319, 'O': 965, 'N': 785, 'c': 318, 't': 563, 'm': 202, 'W': 179, 'H': 391, 'Y': 229, 'P': 389, 'F': 259, 'G': 345, '?': 5, 'S': 1161, 'b': 88, 'h': 299, ' ': 50, 'g': 171, 'L': 745, 'M': 367, 'D': 383, 'd': 257, '$': 46, '5': 77, '4': 44, '.': 95, 'w': 97, 'B': 331, '1': 184, '7': 43, '8': 44, 'V': 158, 'y': 161, 'K': 163, '!': 51, '9': 66, 'z': 12, ';': 3, '#': 16, 'j': 15, "'": 51, 'J': 72, ':': 19, 'x': 27, '%': 28, '/': 24, 'q': 3, 'Q': 19, '(': 6, ')': 5, '\\': 8, '"': 8, 'Â´': 3, 'Z': 29, '&': 9, 'Ã‰': 1, '@': 4, '=': 1, '+': 1}
è®­ç»ƒé›†+éªŒè¯é›†ä¸­å‡ºç°çš„label
{'[': 2, '0': 232, '6': 44, ']': 2, '2': 139, '-': 87, '3': 69, 'C': 893, 'a': 1200, 'r': 935, 'p': 317, 'k': 137, 'E': 2213, 'X': 181, 'I': 1241, 'T': 1315, 'R': 1262, 'f': 203, 'u': 415, 's': 793, 'i': 924, 'o': 954, 'n': 880, 'l': 555, 'e': 1534, 'v': 169, 'A': 1827, 'U': 467, 'O': 1440, 'N': 1158, 'c': 442, 't': 829, 'm': 278, 'W': 288, 'H': 593, 'Y': 341, 'P': 582, 'F': 402, 'G': 521, '?': 7, 'S': 1748, 'b': 129, 'h': 417, ' ': 82, 'g': 260, 'L': 1120, 'M': 536, 'D': 548, 'd': 367, '$': 57, '5': 100, '4': 53, '.': 132, 'w': 136, 'B': 468, '1': 228, '7': 60, '8': 51, 'V': 224, 'y': 231, 'K': 253, '!': 65, '9': 76, 'z': 14, ';': 3, '#': 24, 'j': 19, "'": 70, 'J': 100, ':': 24, 'x': 38, '%': 42, '/': 29, 'q': 3, 'Q': 28, '(': 7, ')': 5, '\\': 8, '"': 8, 'Â´': 3, 'Z': 36, '&': 15, 'Ã‰': 2, '@': 9, '=': 1, '+': 2, 'Ã©': 1}

```
ä¸Šæ–¹ä»£ç ä¸­ï¼Œlbl_cnt_map ä¸ºå­—ç¬¦å‡ºç°æ¬¡æ•°çš„ç»Ÿè®¡å­—å…¸ï¼Œåé¢è¿˜ä¼šç”¨äºå»ºç«‹å­—ç¬¦åŠå…¶idæ˜ å°„å…³ç³»ã€‚

ä»æ•°æ®é›†ç»Ÿè®¡ç»“æœæ¥çœ‹ï¼Œæµ‹è¯•é›†å«æœ‰è®­ç»ƒé›†æ²¡æœ‰å‡ºç°è¿‡çš„å­—ç¬¦ï¼Œä¾‹å¦‚æµ‹è¯•é›†ä¸­åŒ…å«1ä¸ª'Â©'æœªæ›¾åœ¨è®­ç»ƒé›†å‡ºç°ã€‚è¿™ç§æƒ…å†µæ•°é‡ä¸å¤šï¼Œåº”è¯¥é—®é¢˜ä¸å¤§ï¼Œæ‰€ä»¥æ­¤å¤„æœªå¯¹æ•°æ®é›†è¿›è¡Œé¢å¤–å¤„ç†(ä½†æ˜¯æœ‰æ„è¯†çš„è¿›è¡Œè¿™ç§è®­ç»ƒé›†å’Œæµ‹è¯•é›†æ˜¯å¦å­˜åœ¨diffçš„æ£€æŸ¥æ˜¯å¿…è¦çš„)ã€‚



### 3ã€charå’Œidçš„æ˜ å°„å­—å…¸æ„å»º
åœ¨æœ¬æ–‡OCRä»»åŠ¡ä¸­ï¼Œéœ€è¦å¯¹å›¾ç‰‡ä¸­çš„æ¯ä¸ªå­—ç¬¦è¿›è¡Œé¢„æµ‹ï¼Œä¸ºäº†è¾¾åˆ°è¿™ä¸ªç›®çš„ï¼Œé¦–å…ˆå°±éœ€è¦å»ºç«‹ä¸€ä¸ªå­—ç¬¦ä¸å…¶idçš„æ˜ å°„å…³ç³»ï¼Œå°†æ–‡æœ¬ä¿¡æ¯è½¬åŒ–ä¸ºå¯ä¾›æ¨¡å‹è¯»å–çš„æ•°å­—ä¿¡æ¯ï¼Œè¿™ä¸€æ­¥ç±»ä¼¼NLPä¸­å»ºç«‹è¯­æ–™åº“ã€‚

åœ¨æ„å»ºæ˜ å°„å…³ç³»æ—¶ï¼Œé™¤äº†è®°å½•æ‰€æœ‰æ ‡ç­¾æ–‡ä»¶ä¸­å‡ºç°çš„å­—ç¬¦å¤–ï¼Œè¿˜éœ€è¦åˆå§‹åŒ–ä¸‰ä¸ªç‰¹æ®Šå­—ç¬¦ï¼Œåˆ†åˆ«ç”¨æ¥ä»£è¡¨ä¸€ä¸ª **å¥å­èµ·å§‹ç¬¦ã€å¥å­ç»ˆæ­¢ç¬¦å’Œå¡«å……(Padding)** æ ‡è¯†ç¬¦ã€‚ç›¸ä¿¡ç»è¿‡6.1èŠ‚çš„ä»‹ç»ä½ èƒ½å¤Ÿæ˜ç™½è¿™3ç§ç‰¹æ®Šå­—ç¬¦çš„ä½œç”¨ï¼Œåé¢datasetæ„å»ºéƒ¨åˆ†çš„è®²è§£ä¹Ÿè¿˜ä¼šå†æ¬¡æåˆ°ã€‚

è„šæœ¬è¿è¡Œåï¼Œæ‰€æœ‰å­—ç¬¦çš„æ˜ å°„å…³ç³»å°†ä¼šä¿å­˜åœ¨ lbl2id_map.txtæ–‡ä»¶ä¸­ã€‚

æ„å»ºæ˜ å°„ä»£ç ï¼š
```python
 # æ„é€  label - id ä¹‹é—´çš„æ˜ å°„
    print("\n\næ„é€  label - id ä¹‹é—´çš„æ˜ å°„")
    lbl2id_map = dict()
    # åˆå§‹åŒ–ä¸¤ä¸ªç‰¹æ®Šå­—ç¬¦
    lbl2id_map['ğŸ¤'] = 0    # paddingæ ‡è¯†ç¬¦
    lbl2id_map['â– '] = 1    # å¥å­èµ·å§‹ç¬¦
    lbl2id_map['â–¡'] = 2    # å¥å­ç»“æŸç¬¦
    # ç”Ÿæˆå…¶ä½™labelçš„idæ˜ å°„å…³ç³»
    cur_id = 3
    for lbl in lbl_cnt_map.keys():
        lbl2id_map[lbl] = cur_id
        cur_id += 1
    # ä¿å­˜ label - id ä¹‹é—´çš„æ˜ å°„
    with open(lbl2id_map_path, 'w', encoding='utf-8') as writer:
        for lbl in lbl2id_map.keys():
            cur_id = lbl2id_map[lbl]
            print(lbl, cur_id)
            line = lbl + '\t' + str(cur_id) + '\n'
            writer.write(line)
```
æ„å»ºå‡ºæ¥çš„æ˜ å°„å¦‚ä¸‹ï¼š
```
ğŸ¤ 0
â–  1
â–¡ 2
[ 3
0 4
6 5
] 6
2 7
- 8
3 9
C 10
```

è¯»å–label-idæ˜ å°„å…³ç³»è®°å½•æ–‡ä»¶
```python
def load_lbl2id_map(lbl2id_map_path):
    """
    è¯»å–label-idæ˜ å°„å…³ç³»è®°å½•æ–‡ä»¶
    """
    lbl2id_map = dict()
    id2lbl_map = dict()
    with open(lbl2id_map_path, 'r', encoding='utf-8') as reader:
        for line in reader:
            items = line.rstrip().split('\t')
            label = items[0]
            cur_id = int(items[1])
            lbl2id_map[label] = cur_id
            id2lbl_map[cur_id] = label
    return lbl2id_map, id2lbl_map
```
### 4ã€å›¾åƒå°ºå¯¸çš„åˆ†æ
```python
print("\n\n åˆ†ææ•°æ®é›†å›¾ç‰‡å°ºå¯¸")
    min_h = 1e10
    min_w = 1e10
    max_h = -1
    max_w = -1
    min_ratio = 1e10
    max_ratio = 0
    for img_name in os.listdir(train_img_dir):
        img_path = os.path.join(train_img_dir, img_name)
        img = cv2.imread(img_path)
        h, w = img.shape[:2]
        ratio = w / h
        min_h = min_h if min_h <= h else h
        max_h = max_h if max_h >= h else h
        min_w = min_w if min_w <= w else w
        max_w = max_w if max_w >= w else w
        min_ratio = min_ratio if min_ratio <= ratio else ratio
        max_ratio = max_ratio if max_ratio >= ratio else ratio
    print("min_h", min_h)
    print("max_h", max_h)
    print("min_w", min_w)
    print("max_w", max_w)
    print("min_ratio", min_ratio)
    print("max_ratio", max_ratio)
```
ç»“æœå¦‚ä¸‹ï¼š
```
åˆ†ææ•°æ®é›†å›¾ç‰‡å°ºå¯¸
min_h 9
max_h 295
min_w 16
max_w 628
min_ratio 0.6666666666666666
max_ratio 8.619047619047619
```
## å°†transformerå¼•å…¥OCR
transformerå¹¿æ³›åº”ç”¨äºNLPé¢†åŸŸä¸­ï¼Œå¯ä»¥è§£å†³ç±»ä¼¼æœºå™¨ç¿»è¯‘è¿™ç§`sequence to sequence`é—®é¢˜ï¼Œå¦‚ä¸‹å›¾
![sts](./img/sts.jpg)
å¯¹äºocré—®é¢˜ï¼Œæˆ‘ä»¬å¸Œæœ›æŠŠ![241](./img/data_share.png)è¯†åˆ«ä¸ºâ€œShareâ€,å¯ä»¥è®¤ä¸ºæ˜¯ä¸€ä¸ª `image to sequence`é—®é¢˜ã€‚**å¦‚æœè®©imageå˜ä¸ºsequenceï¼Œ é‚£ä¹ˆocrä»»åŠ¡ä¹Ÿå°±å˜æˆäº†ä¸€ä¸ªsequence to sequenceé—®é¢˜ï¼Œä½¿ç”¨transformerè§£å†³ä¹Ÿå°±åˆç†äº†ã€‚** å‰©ä¸‹çš„é—®é¢˜å°±æ˜¯å¦‚ä½•å°†å›¾ç‰‡ä¿¡æ¯æ„é€ æˆtransformeræƒ³è¦çš„ï¼Œç±»ä¼¼äº word embedding å½¢å¼çš„è¾“å…¥ã€‚

å›åˆ°æˆ‘ä»¬çš„ä»»åŠ¡ï¼Œæ—¢ç„¶å¾…é¢„æµ‹çš„å›¾ç‰‡éƒ½æ˜¯é•¿æ¡çŠ¶çš„ï¼Œæ–‡å­—åŸºæœ¬éƒ½æ˜¯æ°´å¹³æ’åˆ—ï¼Œé‚£ä¹ˆæˆ‘ä»¬å°†ç‰¹å¾å›¾æ²¿æ°´å¹³æ–¹å‘è¿›è¡Œæ•´åˆï¼Œå¾—åˆ°çš„æ¯ä¸€ä¸ªembeddingå¯ä»¥è®¤ä¸ºæ˜¯å›¾ç‰‡çºµå‘çš„æŸä¸ªåˆ‡ç‰‡çš„ç‰¹å¾ï¼Œå°†è¿™æ ·çš„ç‰¹å¾åºåˆ—äº¤ç»™transformerï¼Œåˆ©ç”¨å…¶å¼ºå¤§çš„attentionèƒ½åŠ›æ¥å®Œæˆé¢„æµ‹ã€‚

å› æ­¤ï¼ŒåŸºäºä»¥ä¸Šåˆ†æï¼Œæˆ‘ä»¬å°†æ¨¡å‹æ¡†æ¶çš„pipelineå®šä¹‰ä¸ºä¸‹å›¾æ‰€ç¤ºçš„å½¢å¼ï¼š
![ocr](./img/ocr_by_transformer.png)

é€šè¿‡è§‚å¯Ÿä¸Šå›¾å¯ä»¥å‘ç°ï¼Œæ•´ä¸ªpipelineå’Œåˆ©ç”¨transformerè®­ç»ƒæœºå™¨ç¿»è¯‘çš„æµç¨‹æ˜¯åŸºæœ¬ä¸€è‡´çš„ï¼Œä¹‹é—´çš„å·®å¼‚ä¸»è¦æ˜¯å¤šäº† **å€ŸåŠ©ä¸€ä¸ªCNNç½‘ç»œä½œä¸ºbackboneæå–å›¾åƒç‰¹å¾å¾—åˆ°input embeddingçš„è¿‡ç¨‹ã€‚**

å…³äºæ„é€ transformerçš„è¾“å…¥embeddingè¿™éƒ¨åˆ†çš„è®¾è®¡ï¼Œæ˜¯æœ¬æ–‡çš„é‡ç‚¹ï¼Œä¹Ÿæ˜¯æ•´ä¸ªç®—æ³•èƒ½å¤Ÿworkçš„å…³é”®ã€‚åæ–‡ä¼šç»“åˆä»£ç ï¼Œå¯¹ä¸Šé¢ç¤ºæ„å›¾ä¸­å±•ç¤ºçš„ç›¸å…³ç»†èŠ‚è¿›è¡Œå±•å¼€è®²è§£.

## è®­ç»ƒæ¡†æ¶ä»£ç è¯¦è§£

è®­ç»ƒæ¡†æ¶ç›¸å…³ä»£ç å®ç°åœ¨ ocr_by_transformer.py æ–‡ä»¶ä¸­
ä¸‹é¢å¼€å§‹é€æ­¥è®²è§£ä»£ç ï¼Œä¸»è¦æœ‰ä»¥ä¸‹å‡ ä¸ªéƒ¨åˆ†ï¼š
- æ„å»ºdataset â†’ å›¾åƒé¢„å¤„ç†ã€labelå¤„ç†ç­‰
- æ¨¡å‹æ„å»º â†’ backbone + transformer
- æ¨¡å‹è®­ç»ƒ
- æ¨ç† â†’ è´ªå¿ƒè§£ç 

### 1ã€å‡†å¤‡å·¥ä½œ
å¯¼å…¥åº“å¹¶è®¾ç½®åŸºç¡€å‚æ•°
```python

import os
import time
import copy
import numpy as np
from PIL import Image
import torch
import torch.nn as nn
from torch.autograd import Variable
import torchvision.models as models
import torchvision.transforms as transforms
from analysis_recognition_dataset import load_lbl2id_map, statistics_max_len_label
from my_transformer import *
from train_utils import *

import warnings
warnings.filterwarnings("ignore")

base_data_dir = '../ICDAR_2015'
device = torch.device("cuda")
nrof_epochs = 100
batch_size = 64 #è¿™é‡Œç¬”è€…GPUä¸ºä¸¤å—3090ï¼Œå¦‚æœè®¾å¤‡æ˜¾å­˜ä¸å¤Ÿå¯ä»¥é€‚å½“è°ƒå°
model_save_path = "../model_path/orc_model.pth"
```
è¯»å–å›¾åƒlabelä¸­çš„å­—ç¬¦ä¸å…¶idçš„æ˜ å°„å­—å…¸
```python
lbl2id_map_path = os.path.join(base_data_dir, 'lbl2id_map.txt')
lbl2id_map, id2lbl_map = load_lbl2id_map(lbl2id_map_path)

train_lbl_path = os.path.join(base_data_dir, 'train_gt.txt')
valid_lbl_path = os.path.join(base_data_dir, 'valid_gt.txt')
train_max_label_len = statistics_max_len_label(train_lbl_path)
valid_max_label_len = statistics_max_len_label(valid_lbl_path)
sequence_len = max(train_max_label_len, valid_max_label_len)
```
### 2.Datasetæ„å»º
#### 2.1 å›¾ç‰‡é¢„å¤„ç†æ–¹æ¡ˆ
å‡è®¾å›¾ç‰‡å°ºå¯¸ä¸º[batch_size, $3, H_i, W_i$]
ç»è¿‡ç½‘ç»œåçš„ç‰¹å¾å›¾å°ºå¯¸ä¸º[batch_size, $C_f, H_f, W_f$]
åŸºäºä¹‹å‰å¯¹äºæ•°æ®é›†çš„åˆ†æï¼Œå›¾ç‰‡åŸºæœ¬éƒ½æ˜¯æ°´å¹³é•¿æ¡çŠ¶çš„ï¼Œå›¾åƒå†…å®¹æ˜¯æ°´å¹³æ’åˆ—çš„å­—ç¬¦ç»„æˆçš„å•è¯ã€‚é‚£ä¹ˆå›¾ç‰‡ç©ºé—´ä¸ŠåŒä¸€çºµå‘åˆ‡ç‰‡çš„ä½ç½®ï¼ŒåŸºæœ¬åªæœ‰ä¸€ä¸ªå­—ç¬¦ï¼Œå› æ­¤çºµå‘åˆ†è¾¨ç‡ä¸éœ€è¦å¾ˆå¤§ï¼Œé‚£ä¹ˆå–$H_f = 1$ã€‚è€Œæ¨ªå‘åˆ†è¾¨ç‡éœ€è¦å¤§ä¸€äº›ï¼Œæˆ‘ä»¬éœ€è¦ä¸åŒembeddingæ¥ç¼–ç æ°´å¹³æ–¹å‘ä¸Šä¸åŒå­—ç¬¦çš„ç‰¹å¾ã€‚
![adf](./img/img2feature.jpg)
æˆ‘ä»¬é‡‡ç”¨æœ€ç»å…¸çš„**resnet18ç½‘ç»œæ¥ä½œä¸ºbackbone,** ç”±äºå…¶ä¸‹é‡‡æ ·å€æ•°ä¸º32ï¼Œæœ€åä¸€å±‚ç‰¹å¾å›¾channelæ•°ä¸º512ï¼Œé‚£ä¹ˆ:
$H_i = H_f * 32 = 32$
$C_f = 512$
æœ‰ä¸¤ç§æ–¹æ¡ˆæ¥ç¡®å®šè¾“å…¥å›¾ç‰‡çš„å®½åº¦ï¼š
![a](./img/two_resize.jpg)
- **æ–¹æ³•ä¸€ï¼š** è®¾å®šä¸€ä¸ªå›ºå®šå°ºå¯¸ï¼Œå°†å›¾åƒä¿æŒå…¶å®½é«˜æ¯”è¿›è¡Œresizeï¼Œå³ä¾§ç©ºä½™åŒºåŸŸè¿›è¡Œpadding
- **æ–¹æ³•äºŒï¼š** ç›´æ¥å°†åŸå§‹å›¾åƒå¼ºåˆ¶resizeåˆ°ä¸€ä¸ªé¢„è®¾çš„å›ºå®šå°ºå¯¸

è¿™é‡Œé€‰æ‹©æ–¹æ³•ä¸€ï¼Œå› ä¸ºå›¾ç‰‡çš„å®½é«˜æ¯”å’Œå›¾ç‰‡ä¸­å•è¯çš„å­—ç¬¦æ•°é‡æ˜¯å¤§è‡´å‘ˆæ­£æ¯”çš„ï¼Œå¦‚æœé¢„å¤„ç†æ—¶ä¿æŒä½åŸå›¾ç‰‡çš„å®½é«˜æ¯”ï¼Œé‚£ä¹ˆç‰¹å¾å›¾ä¸Šæ¯ä¸€ä¸ªåƒç´ å¯¹åº”åŸå›¾ä¸Šå­—ç¬¦åŒºåŸŸçš„èŒƒå›´å°±æ˜¯åŸºæœ¬ç¨³å®šçš„ï¼Œè¿™æ ·æˆ–è®¸æœ‰æ›´å¥½çš„é¢„æµ‹æ•ˆæœã€‚
è¿™é‡Œè¿˜æœ‰ä¸ªç»†èŠ‚ï¼Œè§‚å¯Ÿä¸Šå›¾ä½ ä¼šå‘ç°ï¼Œæ¯ä¸ªå®½ï¼šé«˜=1:1çš„åŒºåŸŸå†…ï¼ŒåŸºæœ¬éƒ½åˆ†å¸ƒç€2-3ä¸ªå­—ç¬¦ï¼Œå› æ­¤æˆ‘ä»¬å®é™…æ“ä½œæ—¶ä¹Ÿæ²¡æœ‰ä¸¥æ ¼çš„ä¿æŒå®½é«˜æ¯”ä¸å˜ï¼Œè€Œæ˜¯å°†å®½é«˜æ¯”æå‡äº†3å€ï¼Œå³å…ˆå°†åŸå§‹å›¾ç‰‡å®½åº¦æ‹‰é•¿åˆ°åŸæ¥çš„3å€ï¼Œå†ä¿æŒå®½é«˜æ¯”ï¼Œå°†é«˜resizeåˆ°32ã€‚
>è¿™æ ·åšçš„ç›®çš„æ˜¯è®©å›¾ç‰‡ä¸Šæ¯ä¸€ä¸ªå­—ç¬¦ï¼Œéƒ½æœ‰è‡³å°‘ä¸€ä¸ªç‰¹å¾å›¾ä¸Šçš„åƒç´ ä¸ä¹‹å¯¹åº”ï¼Œè€Œä¸æ˜¯ç‰¹å¾å›¾å®½ç»´åº¦ä¸Šä¸€ä¸ªåƒç´ ï¼ŒåŒæ—¶ç¼–ç äº†åŸå›¾ä¸­çš„å¤šä¸ªå­—ç¬¦çš„ä¿¡æ¯ï¼Œè¿™æ ·æˆ‘è®¤ä¸ºä¼šå¯¹transformerçš„é¢„æµ‹å¸¦æ¥ä¸å¿…è¦çš„å›°éš¾

ç¡®å®šäº†resizeæ–¹æ¡ˆï¼Œ$Wi$å…·ä½“è®¾ç½®ä¸ºå¤šå°‘å‘¢ï¼Ÿç»“åˆå‰é¢æˆ‘ä»¬å¯¹æ•°æ®é›†åšåˆ†ææ—¶çš„ä¸¤ä¸ªé‡è¦æŒ‡æ ‡ï¼Œæ•°æ®é›†labelä¸­æœ€é•¿å­—ç¬¦æ•°ä¸º21ï¼Œæœ€é•¿çš„å®½é«˜æ¯”8.6ï¼Œæˆ‘ä»¬å°†æœ€ç»ˆçš„å®½é«˜æ¯”è®¾ç½®ä¸º 24:1ï¼Œå› æ­¤æ±‡æ€»ä¸€ä¸‹å„ä¸ªå‚æ•°çš„è®¾ç½®ï¼š
$H_i = H_f * 32 = 32$
$W_i = 24 * H_i = 768$
$C_f = 512, H_f = 1, W_f = 24$

ä»£ç å®ç°ï¼š
```python
w, h = img.size
ration = round((w / h) * 3)
if ration == 0:
    ration = 1
if ration > self.max_ration:
    ration = self.max_ration
h_new = 32
w_new = h_new * ration
img_resize = img.resize((w_new, h_new), Image.BILINEAR)

img_padd = Image.new("RGB", (32*self.max_ration, 32), (0, 0, 0))
img_padd.paste(img_resize, (0, 0))
```
å®Œæ•´ä»£ç ï¼š
```python
class Recognition_Dataset(object):
    def __init__(self, dataset_root_dir, lbl2id_map, sequence_len, max_ration, phase="train", pad=0):

        if phase == 'train':
            self.img_dir = os.path.join(base_data_dir, 'train')
            self.lbl_path = os.path.join(base_data_dir, 'train_gt.txt')
        else:
            self.img_dir = os.path.join(base_data_dir, 'valid')
            self.lbl_path = os.path.join(base_data_dir, 'valid_gt.txt')

        self.lbl2id_map = lbl2id_map
        self.pad = pad
        self.sequence_len = sequence_len
        self.max_ration = max_ration * 3
        self.imgs_list = []
        self.lbls_list = []

        with open(self.lbl_path, 'r', encoding="utf-8") as reader:
            for line in reader:
                items = line.rstrip().split(',')
                img_name = items[0]
                lbl_str = items[1].strip()[1:-1]

                self.imgs_list.append(img_name)
                self.lbls_list.append(lbl_str)

        self.color_trans = transforms.ColorJitter(0.1, 0.1, 0.1)
        self.trans_Normalize = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize([0.485, 0.457, 0.456], [0.229, 0.224, 0.225])
        ])

    def __getitem__(self, index):
        img_name = self.imgs_list[index]
        img_path = os.path.join(self.img_dir, img_name)
        lbl_str = self.lbls_list[index]

        img = Image.open(img_path).convert("RGB")

        w, h = img.size
        ration = round((w / h) * 3)
        if ration == 0:
            ration = 1
        if ration > self.max_ration:
            ration = self.max_ration
        h_new = 32
        w_new = h_new * ration
        img_resize = img.resize((w_new, h_new), Image.BILINEAR)

        img_padd = Image.new("RGB", (32*self.max_ration, 32), (0, 0, 0))
        img_padd.paste(img_resize, (0, 0))

        img_input = self.color_trans(img_padd)
        img_input = self.trans_Normalize(img_input)


        encode_mask = [1]*ration + [0] * (self.max_ration - ration)
        encode_mask = torch.tensor(encode_mask)
        encode_mask = (encode_mask != 0).unsqueeze(0)
        gt = []
        gt.append(1)
        for lbl in lbl_str:
            gt.append(self.lbl2id_map[lbl])
        gt.append(2)
        for i in range(len(lbl_str), self.sequence_len):
            gt.append(0)
        gt = gt[:self.sequence_len]

        decode_in = gt[:-1]
        decode_in = torch.tensor(decode_in)
        decode_out = gt[1:]
        decode_out = torch.tensor(decode_out)
        decode_mask = self.make_std_mask(decode_in, self.pad)
        ntokens = (decode_out != self.pad).data.sum()

        return img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens

    @staticmethod
    def make_std_mask(tgt, pad):
        tgt_mask = (tgt != pad)
        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))
        tgt_mask = tgt_mask.squeeze(0)
        return tgt_mask

    def __len__(self):
        return len(self.imgs_list)
```
ä¸Šé¢çš„ä»£ç è¿˜æœ‰å‡ ä¸ªå’Œlabelå¤„ç†ç›¸å…³çš„ç»†èŠ‚ï¼Œå±äºTransformerè®­ç»ƒç›¸å…³çš„é€»è¾‘ã€‚

**encode_encode_mask**
ç”±äºæˆ‘ä»¬å¯¹å›¾åƒè¿›è¡Œäº†å°ºå¯¸è°ƒæ•´ï¼Œå¹¶æ ¹æ®éœ€æ±‚å¯¹å›¾åƒè¿›è¡Œäº†paddingï¼Œè€Œpaddingçš„ä½ç½®æ˜¯æ²¡æœ‰åŒ…å«æœ‰æ•ˆä¿¡æ¯çš„ï¼Œä¸ºæ­¤éœ€è¦æ ¹æ®paddingæ¯”ä¾‹æ„é€ ç›¸åº”encode_maskï¼Œè®©transformeråœ¨è®¡ç®—æ—¶å¿½ç•¥è¿™éƒ¨åˆ†æ— æ„ä¹‰çš„åŒºåŸŸã€‚
```python
encode_mask = [1]*ration + [0] * (self.max_ration - ration)
encode_mask = torch.tensor(encode_mask)
encode_mask = (encode_mask != 0).unsqueeze(0)
```
**labelå¤„ç†**
ç”±äºæˆ‘ä»¬å¯¹å›¾åƒè¿›è¡Œäº†å°ºå¯¸è°ƒæ•´ï¼Œå¹¶æ ¹æ®éœ€æ±‚å¯¹å›¾åƒè¿›è¡Œäº†paddingï¼Œè€Œpaddingçš„ä½ç½®æ˜¯æ²¡æœ‰åŒ…å«æœ‰æ•ˆä¿¡æ¯çš„ï¼Œä¸ºæ­¤éœ€è¦æ ¹æ®paddingæ¯”ä¾‹æ„é€ ç›¸åº”encode_maskï¼Œè®©transformeråœ¨è®¡ç®—æ—¶å¿½ç•¥è¿™éƒ¨åˆ†æ— æ„ä¹‰çš„åŒºåŸŸã€‚

**decode_mask**
ä¸€èˆ¬çš„åœ¨decoderä¸­æˆ‘ä»¬ä¼šæ ¹æ®labelçš„sequence_lenç”Ÿæˆä¸€ä¸ªä¸Šä¸‰è§’é˜µå½¢å¼çš„maskï¼Œmaskçš„æ¯ä¸€è¡Œä¾¿å¯ä»¥æ§åˆ¶å½“å‰time_stepæ—¶ï¼Œåªå…è®¸decoderè·å–å½“å‰æ­¥æ—¶ä¹‹å‰çš„å­—ç¬¦ä¿¡æ¯ï¼Œè€Œç¦æ­¢è·å–æœªæ¥æ—¶åˆ»çš„å­—ç¬¦ä¿¡æ¯ï¼Œè¿™é˜²æ­¢äº†æ¨¡å‹è®­ç»ƒæ—¶çš„ä½œå¼Šè¡Œä¸ºã€‚

decode_maskç»è¿‡ä¸€ä¸ªç‰¹æ®Šçš„å‡½æ•° make_std_mask() è¿›è¡Œç”Ÿæˆã€‚

åŒæ—¶ï¼Œdecoderçš„labelåˆ¶ä½œåŒæ ·è¦è€ƒè™‘ä¸Šå¯¹paddingçš„éƒ¨åˆ†è¿›è¡Œmaskï¼Œæ‰€ä»¥decode_maskåœ¨labelè¢«paddingå¯¹åº”çš„ä½ç½®å¤„ä¹Ÿåº”è¯¥è¿›è¡Œå†™æˆFalseã€‚
![mask](./img/decode_mask.png)
```py
 def make_std_mask(tgt, pad):
        tgt_mask = (tgt != pad)
        tgt_mask = tgt_mask & Variable(subsequent_mask(tgt.size(-1)).type_as(tgt_mask.data))
        tgt_mask = tgt_mask.squeeze(0)
        return tgt_mask
```
ä»¥ä¸Šæ˜¯æ„å»ºDatasetçš„æ‰€æœ‰ç»†èŠ‚ï¼Œè¿›è€Œæˆ‘ä»¬å¯ä»¥æ„å»ºå‡ºDataLoaderä¾›è®­ç»ƒä½¿ç”¨
```py
max_ratio = 8
train_dataset = Recognition_Dataset(base_data_dir, lbl2id_map, sequence_len, max_ratio, 'train', pad=0)
valid_dataset = Recognition_Dataset(base_data_dir, lbl2id_map, sequence_len, max_ratio, 'valid', pad=0)

train_loader = torch.utils.data.DataLoader(train_dataset,
                                            batch_size=batch_size,
                                            shuffle=True,
                                            num_workers=4)
valid_loader = torch.utils.data.DataLoader(valid_dataset,
                                            batch_size=batch_size,
                                            shuffle=False,
                                            num_workers=4)

```
###3.æ¨¡å‹æ„å»º
ä»£ç é€šè¿‡ make_ocr_model å’Œ OCR_EncoderDecoder ç±»å®Œæˆæ¨¡å‹ç»“æ„æ­å»ºã€‚

å¯ä»¥ä» make_ocr_model è¿™ä¸ªå‡½æ•°çœ‹èµ·ï¼Œè¯¥å‡½æ•°é¦–å…ˆ**è°ƒç”¨äº†pytorchä¸­é¢„è®­ç»ƒçš„Resnet-18ä½œä¸ºbackboneä»¥æå–å›¾åƒç‰¹å¾ï¼Œ** æ­¤å¤„ä¹Ÿå¯ä»¥æ ¹æ®è‡ªå·±éœ€è¦è°ƒæ•´ä¸ºå…¶ä»–çš„ç½‘ç»œï¼Œä½†éœ€è¦**é‡ç‚¹å…³æ³¨çš„æ˜¯ç½‘ç»œçš„ä¸‹é‡‡æ ·å€æ•°ï¼Œä»¥åŠæœ€åä¸€å±‚ç‰¹å¾å›¾çš„channel_numï¼Œç›¸å…³æ¨¡å—çš„å‚æ•°éœ€è¦åŒæ­¥è°ƒæ•´ã€‚** ä¹‹åè°ƒç”¨äº† OCR_EncoderDecoder ç±»å®Œæˆtransformerçš„æ­å»ºã€‚æœ€åå¯¹æ¨¡å‹å‚æ•°è¿›è¡Œåˆå§‹åŒ–ã€‚

åœ¨ OCR_EncoderDecoder ç±»ä¸­ï¼Œè¯¥ç±»ç›¸å½“äºæ˜¯ä¸€ä¸ªtransformerå„åŸºç¡€ç»„ä»¶çš„æ‹¼è£…çº¿ï¼ŒåŒ…æ‹¬ encoder å’Œ decoder ç­‰ï¼Œå…¶åˆå§‹å‚æ•°æ˜¯å·²å­˜åœ¨çš„åŸºæœ¬ç»„ä»¶ï¼Œå…¶åŸºæœ¬ç»„ä»¶ä»£ç éƒ½åœ¨my_transformer.pyæ–‡ä»¶ä¸­ï¼Œæœ¬æ–‡å°†ä¸åœ¨è¿‡å¤šå™è¿°ã€‚

è¿™é‡Œå†æ¥å›é¡¾ä¸€ä¸‹ï¼Œå›¾ç‰‡ç»è¿‡backboneåï¼Œå¦‚ä½•æ„é€ ä¸ºTransformerçš„è¾“å…¥ï¼š

å›¾ç‰‡ç»è¿‡backboneåå°†è¾“å‡ºä¸€ä¸ªç»´åº¦ä¸º [batch_size, 512, 1, 24] çš„ç‰¹å¾å›¾ï¼Œåœ¨ä¸å…³æ³¨batch_sizeçš„å‰æä¸‹ï¼Œæ¯ä¸€å¼ å›¾åƒéƒ½ä¼šå¾—åˆ°å¦‚ä¸‹æ‰€ç¤ºå…·æœ‰512ä¸ªé€šé“çš„1Ã—24çš„ç‰¹å¾å›¾ï¼Œå¦‚å›¾ä¸­çº¢è‰²æ¡†æ ‡æ³¨æ‰€ç¤ºï¼Œå°†ä¸åŒé€šé“ç›¸åŒä½ç½®çš„ç‰¹å¾å€¼æ‹¼æ¥ç»„æˆä¸€ä¸ªæ–°çš„å‘é‡ï¼Œå¹¶ä½œä¸ºä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å…¥ï¼Œæ­¤æ—¶å˜æ„é€ å‡ºäº†ç»´åº¦ä¸º [batch_size, 24, 512] çš„è¾“å…¥ï¼Œæ»¡è¶³Transformerçš„è¾“å…¥è¦æ±‚ã€‚
![trsn](./img/transpose.jpg)
å®Œæ•´ä»£ç ï¼š
```py
class OCR_EncoderDecoder(nn.Module):
    def __init__(self, encoder, decoder, src_embed, src_position, tgt_embed, generator):
        super(OCR_EncoderDecoder, self).__init__()
        self.encoder = encoder
        self.decoder = decoder
        self.src_embed = src_embed
        self.src_position = src_position
        self.tgt_embed = tgt_embed
        self.generator = generator

    def forward(self, src, tgt, src_mask, tgt_mask):
        memory = self.encode(src, src_mask)
        res = self.decode(memory, src_mask, tgt, tgt_mask)
        return res

    def encode(self, src, src_mask):
        src_embedds = self.src_embed(src)
        src_embedds = src_embedds.squeeze(-2)
        src_embedds = src_embedds.permute(0, 2, 1)

        src_embedds = self.src_position(src_embedds)

        return self.encoder(src_embedds, src_mask)


    def decode(self, memory, src_mask, tgt, tgt_mask):
        target_embedds = self.tgt_embed(tgt)
        return self.decoder(target_embedds, memory, src_mask, tgt_mask)



def make_ocr_model(tgt_vocab, N=6, d_model=512, d_ff=2048, h=8, dropout=0.1):
    c = copy.deepcopy

    backbone = models.resnet18(pretrained=True)
    backbone = nn.Sequential(*list(backbone.children())[:-2])

    attn = MultiHeadedAttention(h, d_model)
    ff = PositionwiseFeedForward(d_model, d_ff, dropout)
    position = PositionalEncoding(d_model, dropout)
    model = OCR_EncoderDecoder(
        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),
        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),
        backbone,
        c(position),
        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),
        Generator(d_model, tgt_vocab))

    for child in model.children():
        if child is backbone:
            for param in child.parameters():
                param.requires_grad = False
            continue
        for p in child.parameters():
            if p.dim() > 1:
                nn.init.xavier_uniform_(p)
    return model
```
é€šè¿‡ä¸Šé¢çš„ç±»å’Œå‡½æ•°ï¼Œå¯ä»¥æ–¹ä¾¿çš„æ„å»ºtransformeræ¨¡å‹ï¼š
```py
    tgt_vocab = len(lbl2id_map.keys())
    d_model = 512
    ocr_model = make_ocr_model(tgt_vocab, N=5, d_model=d_model, d_ff=2048, h=8, dropout=0.1)
    ocr_model.to(device)
```
### 4.æ¨¡å‹è®­ç»ƒ
æ¨¡å‹è®­ç»ƒä¹‹å‰ï¼Œè¿˜éœ€è¦å®šä¹‰**æ¨¡å‹è¯„åˆ¤å‡†åˆ™ã€** **è¿­ä»£ä¼˜åŒ–å™¨**ç­‰ã€‚æœ¬å®éªŒåœ¨è®­ç»ƒæ—¶ï¼Œä½¿ç”¨äº†**æ ‡ç­¾å¹³æ»‘ï¼ˆlabel smoothingï¼‰ã€ç½‘ç»œè®­ç»ƒçƒ­èº«ï¼ˆwarmupï¼‰** ç­‰ç­–ç•¥ï¼Œä»¥ä¸Šç­–ç•¥çš„è°ƒç”¨å‡½æ•°å‡åœ¨train_utils.pyæ–‡ä»¶ä¸­ï¼Œæ­¤å¤„ä¸æ¶‰åŠä»¥ä¸Šä¸¤ç§æ–¹æ³•çš„åŸç†åŠä»£ç å®ç°ã€‚

label smoothingå¯ä»¥å°†åŸå§‹çš„ç¡¬æ ‡ç­¾è½¬åŒ–ä¸ºè½¯æ ‡ç­¾ï¼Œä»è€Œå¢åŠ æ¨¡å‹çš„å®¹é”™ç‡ï¼Œæå‡æ¨¡å‹æ³›åŒ–èƒ½åŠ›ã€‚ä»£ç ä¸­ LabelSmoothing() å‡½æ•°å®ç°äº†label smoothingï¼ŒåŒæ—¶å†…éƒ¨ä½¿ç”¨äº†ç›¸å¯¹ç†µå‡½æ•°è®¡ç®—äº†é¢„æµ‹å€¼ä¸çœŸå®å€¼ä¹‹é—´çš„æŸå¤±ã€‚
```py
class LabelSmoothing(nn.Module):
    "Implement label smoothing."

    def __init__(self, size, padding_idx, smoothing=0.0):
        super(LabelSmoothing, self).__init__()
        self.criterion = nn.KLDivLoss(reduction='sum')
        self.padding_idx = padding_idx
        self.confidence = 1.0 - smoothing
        self.smoothing = smoothing
        self.size = size
        self.true_dist = None

    def forward(self, x, target):
        assert x.size(1) == self.size
        true_dist = x.data.clone()
        true_dist.fill_(self.smoothing / (self.size - 2))
        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)
        true_dist[:, self.padding_idx] = 0
        mask = torch.nonzero(target.data == self.padding_idx)
        if mask.dim() > 0:
            true_dist.index_fill_(0, mask.squeeze(), 0.0)
        self.true_dist = true_dist
        return self.criterion(x, Variable(true_dist, requires_grad=False))
```

warmupç­–ç•¥èƒ½å¤Ÿæœ‰æ•ˆæ§åˆ¶æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¼˜åŒ–å™¨å­¦ä¹ ç‡ï¼Œè‡ªåŠ¨åŒ–çš„å®ç°æ¨¡å‹å­¦ä¹ ç‡ç”±å°å¢å¤§å†é€æ¸ä¸‹é™çš„æ§åˆ¶ï¼Œå¸®åŠ©æ¨¡å‹åœ¨è®­ç»ƒæ—¶æ›´åŠ ç¨³å®šï¼Œå®ç°æŸå¤±çš„å¿«é€Ÿæ”¶æ•›ã€‚ä»£ç ä¸­ NoamOpt() å‡½æ•°å®ç°äº†warmupæ§åˆ¶ï¼Œé‡‡ç”¨çš„Adamä¼˜åŒ–å™¨ï¼Œå®ç°å­¦ä¹ ç‡éšè¿­ä»£æ¬¡æ•°çš„è‡ªåŠ¨è°ƒæ•´ã€‚
```py
class NoamOpt:
    "Optim wrapper that implements rate."

    def __init__(self, model_size, factor, warmup, optimizer):
        self.optimizer = optimizer
        self._step = 0
        self.warmup = warmup
        self.factor = factor
        self.model_size = model_size
        self._rate = 0

    def step(self):
        "Update parameters and rate"
        self._step += 1
        rate = self.rate()
        for p in self.optimizer.param_groups:
            p['lr'] = rate
        self._rate = rate
        self.optimizer.step()

    def rate(self, step=None):
        "Implement `lrate` above"
        if step is None:
            step = self._step
        return self.factor * \
               (self.model_size ** (-0.5) *
                min(step ** (-0.5), step * self.warmup ** (-1.5)))
```
è¿›è¡Œä¸Šè¿°æ“ä½œï¼š
```py
criterion = LabelSmoothing(size=tgt_vocab, padding_idx=0, smoothing=0.0)  
optimizer = torch.optim.Adam(ocr_model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9)
model_opt = NoamOpt(d_model, 1, 400, optimizer)
```
**SimpleLossCompute()** ç±»å®ç°äº†transformerè¾“å‡ºç»“æœçš„lossè®¡ç®—ã€‚åœ¨ä½¿ç”¨è¯¥ç±»ç›´æ¥è®¡ç®—æ—¶ï¼Œç±»éœ€è¦æ¥æ”¶(x, y, norm)ä¸‰ä¸ªå‚æ•°ï¼Œxä¸ºdecoderè¾“å‡ºçš„ç»“æœï¼Œyä¸ºæ ‡ç­¾æ•°æ®ï¼Œnormä¸ºlossçš„å½’ä¸€åŒ–ç³»æ•°ï¼Œç”¨batchä¸­æ‰€æœ‰æœ‰æ•ˆtokenæ•°å³å¯ã€‚ç”±æ­¤å¯è§ï¼Œæ­¤å¤„æ‰æ­£å®Œæˆtransformeræ‰€æœ‰ç½‘ç»œçš„æ„å»ºï¼Œå®ç°æ•°æ®è®¡ç®—æµçš„æµé€šã€‚
```py
class SimpleLossCompute:
    def __init__(self, generator, criterion, opt=None):
        self.generator = generator
        self.criterion = criterion
        self.opt = opt

    def __call__(self, x, y, norm):

        x = self.generator(x)
        x_ = x.contiguous().view(-1, x.size(-1))
        y_ = y.contiguous().view(-1)
        loss = self.criterion(x_, y_)
        loss /= norm
        loss.backward()
        if self.opt is not None:
            self.opt.step()
            self.opt.optimizer.zero_grad()

        return loss.item() * norm
```
**run_epoch()** å‡½æ•°å†…éƒ¨å®Œæˆäº†ä¸€ä¸ªepochè®­ç»ƒçš„æ‰€æœ‰å·¥ä½œï¼ŒåŒ…æ‹¬æ•°æ®åŠ è½½ã€æ¨¡å‹æ¨ç†ã€æŸå¤±è®¡ç®—ä¸æ–¹å‘ä¼ æ’­ï¼ŒåŒæ—¶å°†è®­ç»ƒè¿‡ç¨‹ä¿¡æ¯è¿›è¡Œæ‰“å°ã€‚
```py
def run_epoch(data_loader, model, loss_compute, device=None):
    start = time.time()
    total_tokens = 0
    total_loss = 0
    tokens = 0
    for i, batch in enumerate(data_loader):
        img_input, encode_mask, decode_in, decode_out, decode_mask, ntokens = batch
        img_input = img_input.to(device)
        encode_mask = encode_mask.to(device)
        decode_in = decode_in.to(device)
        decode_out = decode_out.to(device)
        decode_mask = decode_mask.to(device)
        ntokens = torch.sum(ntokens).to(device)

        out = model.forward(img_input, decode_in, encode_mask, decode_mask)

        loss = loss_compute(out, decode_out, ntokens)
        total_loss += loss
        total_tokens += tokens
        tokens += ntokens

        if i % 50 == 1:
            elapsed = time.time() - start
            print("Epoch Step: %d Loss: %f Tokens per Sec: %f" %
                  (i, loss / ntokens, tokens / elapsed))
            start = time.time()
            tokens = 0
    return total_loss / total_tokens
```
### 5. è´ªå¿ƒè§£ç 
æˆ‘ä»¬ä½¿ç”¨æœ€ç®€å•çš„è´ªå¿ƒè§£ç ç›´æ¥è¿›è¡ŒOCRç»“æœé¢„æµ‹ã€‚å› ä¸ºæ¨¡å‹æ¯ä¸€æ¬¡åªä¼šäº§ç”Ÿä¸€ä¸ªè¾“å‡ºï¼Œæˆ‘ä»¬é€‰æ‹©è¾“å‡ºçš„æ¦‚ç‡åˆ†å¸ƒä¸­çš„æœ€é«˜æ¦‚ç‡å¯¹åº”çš„å­—ç¬¦ä¸ºæœ¬æ¬¡é¢„æµ‹çš„ç»“æœï¼Œç„¶åé¢„æµ‹ä¸‹ä¸€ä¸ªå­—ç¬¦ï¼Œè¿™å°±æ˜¯æ‰€è°“çš„è´ªå¿ƒè§£ç ï¼Œè§ä»£ç ä¸­ greedy_decode() å‡½æ•°ã€‚

å®éªŒä¸­åˆ†åˆ«å°†æ¯ä¸€å¼ å›¾åƒä½œä¸ºæ¨¡å‹çš„è¾“å…¥ï¼Œé€å¼ è¿›è¡Œè´ªå¿ƒè§£ç ç»Ÿè®¡æ­£ç¡®ç‡ï¼Œå¹¶æœ€ç»ˆç»™å‡ºäº†è®­ç»ƒé›†å’ŒéªŒè¯é›†å„è‡ªçš„é¢„æµ‹å‡†ç¡®ç‡ã€‚

**greedy_decode()** å‡½æ•°å®ç°ã€‚
```py
def greedy_decode(model, src, src_mask, max_len, start_symbol, end_symbol):
    memory = model.encode(src, src_mask)
    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data).long()
    for i in range(max_len - 1):
        out = model.decode(memory, src_mask,
                           Variable(ys),
                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))
        prob = model.generator(out[:, -1])
        _, next_word = torch.max(prob, dim=1)
        next_word = next_word.data[0]
        next_word = torch.ones(1, 1).type_as(src.data).fill_(next_word).long()
        ys = torch.cat([ys, next_word], dim=1)

        next_word = int(next_word)
        if next_word == end_symbol:
            break
    ys = ys[0, 1:]
    return ys
```
ç»“æœå¦‚ä¸‹ï¼š
```py
greedy decode trainset
----
tensor([43, 34, 11, 51, 23, 21, 35,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0])
tensor([43, 24, 21, 25, 51,  2])
----
tensor([17, 32, 18, 19, 31, 50, 30, 10, 30, 10, 17, 32, 41, 55, 55, 55,  2,  0,
         0,  0])
tensor([17, 32, 18, 19, 31, 50, 30, 10, 30, 10, 17, 32, 41, 55, 55, 55, 55, 55,
        55, 55])
----
tensor([17, 32, 18, 19, 31, 50, 30, 10, 17, 32, 41, 55, 55,  2,  0,  0,  0,  0,
         0,  0])
tensor([17, 32, 18, 19, 31, 50, 30, 10, 17, 32, 41, 55, 55, 55,  2])
----
tensor([49, 49, 29,  2,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0,
         0,  0])
tensor([49, 49, 49, 29,  2])
----
tensor([78, 46, 88,  5, 53, 79, 46,  5, 59,  9,  7, 46,  7, 65,  4,  4,  2,  0,
         0,  0])
tensor([78, 46, 88,  5, 53, 79, 46,  5, 59,  9,  7, 46,  7, 65,  4,  2])
total correct rate of trainset: 99.88441978733242%
```
æœ€åå°†é¢„æµ‹å‡ºæ¥çš„å¼ é‡è¿›è¡Œæ˜ å°„å°±å¯ä»¥è¾¾åˆ°è¯†åˆ«å‡ºçš„å­—ç¬¦äº†ã€‚

## last word
æˆ‘ä»¬ä½¿ç”¨ICDAR2015ä¸­çš„ä¸€ä¸ªå•è¯è¯†åˆ«ä»»åŠ¡æ•°æ®é›†ï¼Œç„¶åå¯¹æ•°æ®çš„ç‰¹ç‚¹è¿›è¡Œäº†ç®€å•åˆ†æï¼Œå¹¶æ„å»ºäº†è¯†åˆ«ç”¨çš„å­—ç¬¦æ˜ å°„å…³ç³»è¡¨ã€‚ä¹‹åï¼Œæˆ‘ä»¬é‡ç‚¹ä»‹ç»äº†å°†transformerå¼•å…¥æ¥è§£å†³OCRä»»åŠ¡çš„åŠ¨æœºä¸æ€è·¯ï¼Œå¹¶ç»“åˆä»£ç è¯¦ç»†ä»‹ç»äº†ç»†èŠ‚ï¼Œæœ€åæˆ‘ä»¬å¤§è‡´è¿‡äº†ä¸€äº›è®­ç»ƒç›¸å…³çš„é€»è¾‘å’Œä»£ç ã€‚

å¤§è‡´ä»‹ç»transformeråœ¨CVé¢†åŸŸçš„åº”ç”¨æ–¹æ³•ï¼Œå¸Œæœ›å¸®åŠ©å¤§å®¶æ‰“å¼€æ€è·¯ã€‚â¤
![](./img/hf.svg)